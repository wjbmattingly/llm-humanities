{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by [William Mattingly](https://www.wjbmattingly.com) for the 2025 Text Analysis Pedagogy Institute, with support from [Constellate](https://constellate.org).\n",
    "<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Leveraging LLMs for Named Entity Recognition\n",
    "\n",
    "In this notebook, we will learn how to leverage LLMs to perform a specific task: Named Entity Recognition (NER). We will also learn some tricks for guiding LLMs to do what we want consistently.\n",
    "\n",
    "Learning Objects\n",
    "\n",
    "- Understanding NER as a task\n",
    "- Understanding the different approaches to NER\n",
    "- Understanding when to use an LLM for NER\n",
    "- Understanding the Limitations of an LLM\n",
    "- Understanding Pydantic and Data Models\n",
    "- Understanding Structured Outputs from LLMs and their Importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (1.73.0)\n",
      "Requirement already satisfied: pydantic in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (2.11.3)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: spacy in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (3.8.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from pydantic) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from pydantic) (0.4.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from spacy) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install openai pydantic datasets spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to American Stories Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wjbmattingly/american-stories-sample-tap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article_id', 'newspaper_name', 'edition', 'date', 'page', 'headline', 'byline', 'article'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British. Surrender to Nozs Dotes\n",
      "\n",
      " Back to Three Post-ANor Decisions; Hitler Saw They Would Not Fight After They Let Itoly. Toke Ethiopio;\n",
      "\n",
      " After France Let Them Toke Rhnelond;; M w And When Franco Was Not Checked\n",
      "\n",
      " WASHINGTON, September g. - Here are two. tl1vb-naN sketches of history which should be kept in Av1\n"
     ]
    }
   ],
   "source": [
    "print(dataset[2]['article'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ways to Solve NER\n",
    "\n",
    "Named Entity Recognition (NER) can be approached through several different methods, each with its own strengths and use cases. One of the most basic approaches is using rule-based systems, where human experts define explicit patterns and rules to identify entities. Rule-based systems are highly precise for specific domains, easy to understand and debug, and don't require training data. However, they can be labor-intensive to create and maintain, and may struggle with novel or ambiguous cases that don't match predefined rules.\n",
    "\n",
    "Regular expressions (regex) offer another approach to NER by defining text patterns to match entities. Regex is powerful for identifying entities with consistent formats like phone numbers, email addresses, or dates. It's fast, lightweight, and gives precise control over pattern matching. However, like rule-based systems, regex patterns can be complex to maintain and may not handle variations or context well.\n",
    "\n",
    "Machine learning approaches, particularly deep learning models, have become increasingly popular for NER. These systems learn to identify entities from large amounts of annotated training data, capturing complex patterns and contextual relationships that would be difficult to define manually. ML models can generalize well to new examples and handle ambiguous cases, but they require significant training data and computational resources.\n",
    "\n",
    "Spacy stands out as a versatile NLP library because it combines all these approaches. You can use Spacy's statistical models for machine learning-based NER, define rule-based patterns with its matcher components, and incorporate regex patterns where needed. This flexibility allows developers to choose the best approach for their specific use case, or even combine multiple approaches for optimal results. Additionally, Spacy's efficient implementation and easy-to-use API make it practical for both small and large-scale applications.\n",
    "\n",
    "Let's take a look at what NER looks like in Python. First, we will import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will load the small English pipeline that has a machine learning NER component. We downloaded this earlier in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will run it over one of our articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(dataset[2]['article'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose this article specifically because it is messy and represents real-world data, espcially humanities data. What are some of the problems you can identify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British. Surrender to Nozs Dotes\n",
      "\n",
      " Back to Three Post-ANor Decisions; Hitler Saw They Would Not Fight After They Let Itoly. Toke Ethiopio;\n",
      "\n",
      " After France Let Them Toke Rhnelond;; M w And When Franco Was Not Checked\n",
      "\n",
      " WASHINGTON, September g. - Here are two. tl1vb-naN sketches of history which should be kept in Av1\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at the NER, we can print off the entities and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British NORP\n",
      "Surrender PERSON\n",
      "Three CARDINAL\n",
      "Toke Ethiopio PERSON\n",
      "France GPE\n",
      "Franco PERSON\n",
      "WASHINGTON GPE\n",
      "two CARDINAL\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are... not great. This is expected. The reason they are bad is because we are using the small English model. If we used a larger model, things would improve. The main reason we are seeing based results, though, is because of data quality. Our data was OCRed where mistakes in the transcription remain. This is a very common problem. One of the biggest advantages of LLMs is that their robust knowledge of language (even in it's messy forms!) allows them to predict well on these types of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an LLM to Solve NER\n",
    "\n",
    "Large Language Models (LLMs) offer several compelling advantages for Named Entity Recognition (NER) tasks. Their broad knowledge base and contextual understanding allow them to identify entities even in noisy or informal text where traditional NER models might fail. LLMs can also recognize novel or rare entities that weren't in their training data, and they can adapt to domain-specific terminology without requiring explicit retraining. This flexibility is particularly valuable when working with historical texts, social media content, or specialized technical documents.\n",
    "\n",
    "However, LLMs come with significant drawbacks that need to be considered. They are computationally expensive and typically require API calls to cloud services, making them costly for large-scale processing. Their responses can also be inconsistent between runs, making them less suitable for applications requiring deterministic results. Additionally, the \"black box\" nature of LLMs makes it difficult to understand or debug their decision-making process, unlike traditional NER models where the features and weights are more transparent.\n",
    "\n",
    "When choosing between LLMs and traditional NER approaches, consider your specific use case. LLMs are ideal for projects dealing with messy, inconsistent data or requiring flexible entity recognition across diverse domains. They're also excellent for prototyping or small-scale applications where accuracy is prioritized over speed or cost. However, for large-scale production systems processing millions of documents, or applications requiring consistent, reproducible results, traditional NER models might be more appropriate. The best solution might be a hybrid approach, using LLMs to handle complex cases while relying on traditional NER for straightforward, high-volume processing.\n",
    "\n",
    "To use an LLM, let's first connect to the OpenAI API. We'll first import our libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then connect to the API like we did in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    # api_key=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's craft our first NER Messages list to the model. Here, we define a very clear system prompt and provide a text to exract entities from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_ner_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that identifies named entities in a text.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Here is a text: {dataset[2]['article']}\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use that variable to create a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the named entities identified in the text:\n",
      "\n",
      "1. British\n",
      "2. Nozs Dotes\n",
      "3. Hitler\n",
      "4. Itoly (likely a misspelling of Italy)\n",
      "5. Ethiopio (likely a misspelling of Ethiopia)\n",
      "6. France\n",
      "7. Rhnelond (possibly a misspelling of Rhineland)\n",
      "8. Franco\n",
      "9. WASHINGTON\n",
      "10. September\n",
      "\n",
      "Note: There are several misspellings in the text that seem to be intended to refer to prominent entities. If you can provide more context or corrected spellings, it may help in more accurate identification.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=basic_ner_messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this output looks good (your experience will vary), it has some issues. First, I don't really know what these entities are, so what's a place, person, etc. Second, I don't have a consistent output that I can work with. I don't have structure that allows me to manipulate the data in a specific way. We'll learn how to tackle some of these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EXERCISE 1** (10 minutes)\n",
    "\n",
    "Use this time to test out this approach. Use the prompting and few-shot approaches from last class to find only locations. Try and make those locations output in a consistent format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Pydantic\n",
    "\n",
    "Pydantic is a Python library for data validation and settings management using Python type annotations. It enforces type hints at runtime and provides user-friendly errors when data is invalid.\n",
    "\n",
    "Key features of Pydantic include:\n",
    "- Data validation using Python type annotations\n",
    "- Automatic JSON schema generation\n",
    "- Customizable validation rules\n",
    "- Serialization/deserialization to and from JSON\n",
    "- Integration with FastAPI and other frameworks\n",
    "\n",
    "We'll use Pydantic to create structured outputs from our LLM responses, ensuring the data follows a consistent schema and format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting with Structured Outputs\n",
    "\n",
    "To begin working with Pydantic, we first need to import a data model from it. For our purposes, we will work with the BaseModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BaseModel class is the foundation of Pydantic's data validation system. It allows you to define data models using Python type hints, which Pydantic then uses to validate data at runtime. When you create a class that inherits from BaseModel, you can define attributes with type annotations, and Pydantic will automatically validate any data assigned to those attributes to ensure it matches the specified types. This makes it easy to create structured, type-safe data models that can be used to validate and serialize/deserialize data.\n",
    "\n",
    "Let's create a simple example. Imagine we wanted to just find the locations in the same text as above. We can create a BasicLocation data model like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLocation(BaseModel):\n",
    "    text: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to think about this model is as a dictionary:\n",
    "\n",
    "```python\n",
    "{\"text\": \"\"}\n",
    "```\n",
    "\n",
    "This is essentially the exact same thing as the data model above. Now, how do we use it? Well let's first create a new prompt that will be a bit more specific to finding locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "locations_ner_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that identifies locations in a text. Locations are physical places, like cities, states, countries, etc. They are not people, organizations, or other entities. They are not groups of people, like British or American.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Find only the locations in this text: {dataset[2]['article']}\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go ahead and use the model. Notice here that we are using a different endpoint, specificially `client.beta.chat.completions.parse`. This allows us to pass an extra argument: `response_format` which can take a Pydantic data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"text\":\"Three, ANor, Italy, Ethiopia, France, Rhineland, Washington, Av1\"}\n"
     ]
    }
   ],
   "source": [
    "response = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=locations_ner_messages,\n",
    "    response_format=BasicLocation\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above has some right answers and some wrong answers. This isn't good. We can rerun this several times and we'll get different (and some better) results. But this is impractical for one main reason: the data is just a list of names provided as a single string. This isn't well-structured data for entities. We want our data to be a list of entities, not a continuous string.\n",
    "\n",
    "To fix this, we can make another data model that will be a list of our BasicLocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationList(BaseModel):\n",
    "    locations: list[BasicLocation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"locations\":[{\"text\":\"Italy\"},{\"text\":\"Ethiopia\"},{\"text\":\"France\"},{\"text\":\"Rhineland\"},{\"text\":\"Washington\"}]}\n"
     ]
    }
   ],
   "source": [
    "response = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=locations_ner_messages,\n",
    "    response_format=LocationList\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have something that is starting to look a bit better. Notice that we have a key of \"locations\" which points to a list of dictionaries. These dictionaries all have one key, \"text\" that points to the text of the location. But we have a problem. The data doesn't tell us if this is the corrected OCR or the raw text found inside the article. This can make it challenging to validate any output.\n",
    "\n",
    "To fix this, we can add extra variables to our Location class, such as \"original_text\" and \"corrected_ocr\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"locations\":[{\"original_text\":\"Italy\",\"corrected_ocr\":\"Italy\",\"corrected\":true,\"type\":\"country\"},{\"original_text\":\"Ethiopio\",\"corrected_ocr\":\"Ethiopia\",\"corrected\":true,\"type\":\"country\"},{\"original_text\":\"France\",\"corrected_ocr\":\"France\",\"corrected\":false,\"type\":\"country\"},{\"original_text\":\"Rhnelond\",\"corrected_ocr\":\"Rhineland\",\"corrected\":true,\"type\":\"other\"},{\"original_text\":\"Franco\",\"corrected_ocr\":\"Franco\",\"corrected\":false,\"type\":\"other\"},{\"original_text\":\"WASHINGTON\",\"corrected_ocr\":\"Washington\",\"corrected\":true,\"type\":\"other\"}]}\n"
     ]
    }
   ],
   "source": [
    "class LocationCorrected(BaseModel):\n",
    "    original_text: str\n",
    "    corrected_ocr: str\n",
    "\n",
    "class LocationListCorrected(BaseModel):\n",
    "    locations: list[LocationCorrected]\n",
    "\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=locations_ner_messages,\n",
    "    response_format=Locations\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even modify this further to have a boolean to let us know if the data was in fact corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"locations\":[{\"original_text\":\"British\",\"corrected_ocr\":\"British\",\"was_corrected\":false},{\"original_text\":\"Nozs Dotes\",\"corrected_ocr\":\"Nazi Germany\",\"was_corrected\":true},{\"original_text\":\"Itoly\",\"corrected_ocr\":\"Italy\",\"was_corrected\":true},{\"original_text\":\"Ethiopio\",\"corrected_ocr\":\"Ethiopia\",\"was_corrected\":true},{\"original_text\":\"France\",\"corrected_ocr\":\"France\",\"was_corrected\":false},{\"original_text\":\"Rhnelond\",\"corrected_ocr\":\"Rhineland\",\"was_corrected\":true},{\"original_text\":\"Franco\",\"corrected_ocr\":\"Spain\",\"was_corrected\":true},{\"original_text\":\"WASHINGTON\",\"corrected_ocr\":\"WASHINGTON\",\"was_corrected\":false}]}\n"
     ]
    }
   ],
   "source": [
    "class LocationCorrected(BaseModel):\n",
    "    original_text: str\n",
    "    corrected_ocr: str\n",
    "    was_corrected: bool\n",
    "\n",
    "class LocationListCorrected(BaseModel):\n",
    "    locations: list[LocationCorrected]\n",
    "\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=locations_ner_messages,\n",
    "    response_format=LocationListCorrected\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will this always work? Absolutely not. In fact, if you examine the results above, I'm certain you will see mistakes. And, if you don't then simply run the cell above a few more times you will begin to see a few.\n",
    "\n",
    "Before we move forward, I thought it best to also have you examine the type of data that we see above. This looks like a dictionary, right? But if we examine it, we will find that it is actually a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we could use json.loads to conver the data into JSON or we can use the parsed structured data that the OpenAI API provides us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locations=[Location(original_text='Italy', corrected_ocr='ltoly', corrected=True), Location(original_text='Ethiopia', corrected_ocr='Ethiopio', corrected=True), Location(original_text='France', corrected_ocr='France', corrected=False), Location(original_text='Rhineland', corrected_ocr='Rhnelond', corrected=True), Location(original_text='Washington', corrected_ocr='WASHINGTON', corrected=True)]\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to get more granularity about the data we are extracting. We could add `type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locations=[Location(original_text='Italy', corrected_ocr='Itoly', corrected=True, type='country'), Location(original_text='Ethiopia', corrected_ocr='Ethiopio', corrected=True, type='country'), Location(original_text='France', corrected_ocr='France', corrected=False, type='country'), Location(original_text='Rhineland', corrected_ocr='Rhnelond', corrected=True, type='region'), Location(original_text='WASHINGTON', corrected_ocr='WASHINGTON', corrected=False, type='city')]\n"
     ]
    }
   ],
   "source": [
    "class Location(BaseModel):\n",
    "    original_text: str\n",
    "    corrected_ocr: str\n",
    "    corrected: bool\n",
    "    type: str\n",
    "\n",
    "class Locations(BaseModel):\n",
    "    locations: list[Location]\n",
    "\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=locations_ner_messages,\n",
    "    response_format=Locations\n",
    ")\n",
    "print(response.choices[0].message.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, `type` isn't very clear. What do we mean by `type`? This is where we can further add clarity in our schema by providing the model with descriptions. To get descriptions, we can use the `Field` class from Pydantic and pass a description to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locations=[Location(original_text='Ethiopio', corrected_ocr='Ethiopia', corrected=True, type='country'), Location(original_text='France', corrected_ocr='France', corrected=False, type='country'), Location(original_text='Rhnelond', corrected_ocr='Rhineland', corrected=True, type='other'), Location(original_text='WASHINGTON', corrected_ocr='Washington', corrected=False, type='other')]\n"
     ]
    }
   ],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "class Location(BaseModel):\n",
    "    original_text: str = Field(description=\"The original text of the location\")\n",
    "    corrected_ocr: str = Field(description=\"The corrected OCR of the location\")\n",
    "    corrected: bool = Field(description=\"Whether the location was corrected\")\n",
    "    type: str = Field(description=\"The type of location, either 'city', 'state', 'country', or 'other'\")\n",
    "\n",
    "class Locations(BaseModel):\n",
    "    locations: list[Location] = Field(description=\"A list of locations\")\n",
    "\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=locations_ner_messages,\n",
    "    response_format=Locations\n",
    ")\n",
    "print(response.choices[0].message.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EXERCISE 2** (10 Minutes)\n",
    "\n",
    "Use the article below. Try and create a Pydantic data model that can find and extract the people in the text. If you succeed at this, try and also capture their professional title and roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BY the Associated Press.\n",
      "\n",
      "\n",
      "CHICAGO, Nov. 10.-Assistant\n",
      "Attorney General Thurman Arnold\n",
      "declared yesterday that dominant\n",
      "American business\" was to blame\n",
      "for defense production lag.\n",
      "\n",
      "\n",
      "In an N. B. c. radio address on\n",
      "the University of Chicago Round\n",
      "Table Mr.. Arnold said that \"for\n",
      "the first 10 months our defense\n",
      "effort was hampered by the fear of\n",
      "expansion of the production Of basic\n",
      "materials\"\n",
      "\n",
      "\n",
      "Businessmen, he said, 'indulg-\n",
      "ing in wishful thinking. concealed\n",
      "shortages by overoptimistic predic-\n",
      "tons of supply.\n",
      "\n",
      "\n",
      "Il would still insist that the gen.\n",
      "eral attitude of dominant American\n",
      "business. fearing overproduction\n",
      "after the war, was responsible for\n",
      "this lag in production\"\n",
      "\n",
      "\n",
      "Leo M. Cherne, director Of the\n",
      "Research Institute of America. told\n",
      "the same audience that labor also\n",
      "was partly responsible and urged\n",
      "that some sort of legislation is\n",
      "needed to restrict labor's demands\n",
      "to the purely legitimate needs re-\n",
      "lating to hours. wages and condi-\n",
      "tions of work.\n",
      "\n",
      "\n",
      "Mr. Cherne estimated that Amer\n",
      "ican production at the present rate\n",
      "could not beat Hitler in 10 years.\n",
      "but If so per cent of our energy\n",
      "were devoted to defense We could\n",
      "outproduce Germany in three years\n"
     ]
    }
   ],
   "source": [
    "print(dataset[39]['article'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Outputs and Few-Shot\n",
    "\n",
    "We can improve our outputs even more by providing the model with 1 or more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locations=[Location(original_text='Italy', corrected_ocr='Italy', corrected=True, type='country'), Location(original_text='Ethiopio', corrected_ocr='Ethiopia', corrected=True, type='country'), Location(original_text='France', corrected_ocr='France', corrected=False, type='country'), Location(original_text='Rhnelond', corrected_ocr='Rhineland', corrected=True, type='other'), Location(original_text='Washington', corrected_ocr='Washington', corrected=False, type='other')]\n"
     ]
    }
   ],
   "source": [
    "few_shot_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that identifies locations in a text. Locations are physical places, like cities, states, countries, etc. They are not people, organizations, or other entities. They are not groups of people, like British or American.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Here is a text: The Nazis invaded Paland in 1938. The British were in Britain.\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"[{\\\"original_text\\\": \\\"Paland\\\", \\\"corrected_ocr\\\": \\\"Poland\\\", \\\"corrected\\\": true, \\\"type\\\": \\\"country\\\"}, {\\\"original_text\\\": \\\"Britain\\\", \\\"corrected_ocr\\\": \\\"Britain\\\", \\\"corrected\\\": false, \\\"type\\\": \\\"country\\\"}]\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Find only the locations in this text: {dataset[2]['article']}\"\n",
    "    }\n",
    "]\n",
    "class Location(BaseModel):\n",
    "    original_text: str = Field(description=\"The original text of the location\")\n",
    "    corrected_ocr: str = Field(description=\"The corrected OCR of the location\")\n",
    "    corrected: bool = Field(description=\"Whether the location was corrected\")\n",
    "    type: str = Field(description=\"The type of location, either 'city', 'state', 'country', or 'other'\")\n",
    "\n",
    "class Locations(BaseModel):\n",
    "    locations: list[Location] = Field(description=\"A list of locations\")\n",
    "\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=few_shot_messages,\n",
    "    response_format=Locations\n",
    ")\n",
    "print(response.choices[0].message.parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
