{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by [William Mattingly](https://www.wjbmattingly.com) for the 2025 Text Analysis Pedagogy Institute, with support from [Constellate](https://constellate.org).\n",
    "<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Introduction to LLMs, APIs, and Key Concepts\n",
    "\n",
    "In this notebook we will lay the foundations for the next two lessons on applying large language models (LLMs) within the contex to of the humanities.\n",
    "\n",
    "Learning Objects\n",
    "\n",
    "- Understanding the basics of large language models\n",
    "- Understanding the strengths and weaknesses of LLMs\n",
    "- Understanding how to interact with LLMs via APIs\n",
    "- Learn how to setup an OpenAI account and create an API key\n",
    "- Learn how to make an API call with your OpenAI account in Python\n",
    "- Understanding the importance of examples\n",
    "    - Zero-shot Classification\n",
    "    - Few-shot Classification\n",
    "- Understanding the importance and affect of context\n",
    "- Practical Applications of LLMs in the Humanities (if there's extra time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.73.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/python312/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.8-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.33.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading openai-1.73.0-py3-none-any.whl (644 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.4/644.4 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.8-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.9.0-cp312-cp312-macosx_11_0_arm64.whl (319 kB)\n",
      "Downloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.1-cp312-cp312-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: typing-inspection, sniffio, pydantic-core, jiter, h11, distro, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.9.0 distro-1.9.0 h11-0.14.0 httpcore-1.0.8 httpx-0.28.1 jiter-0.9.0 openai-1.73.0 pydantic-2.11.3 pydantic-core-2.33.1 sniffio-1.3.1 typing-inspection-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Introduction to Large Language Models\n",
    "\n",
    "Large Language Models (LLMs) represent the cutting edge of natural language processing. These sophisticated AI systems are designed to comprehend, generate, and manipulate human language with proficiency. Built on deep learning architectures, typically employing transformer models, LLMs are trained on vast corpora of text data (trillions of tokens). This extensive training allows them to capture the subtle nuances of language, including complex grammatical structures, contextual meanings, and even rudimentary reasoning capabilities.\n",
    "\n",
    "LLMs are very versatile and can be used to solve (or partially solve) many NLP problems. From translation to summarization, from question-answering to creative writing, these models demonstrate a wide-ranging applicability across numerous language tasks. You might be familiar with some popular examples, such as the GPT (Generative Pre-trained Transformer) series, the basis for ChatGPT. These models have played a pivotal role in revolutionizing natural language processing, enabling more human-like text generation and understanding than ever before.\n",
    "\n",
    "\n",
    "### Strengths and Weaknesses of LLMs\n",
    "\n",
    "Like any technology, LLMs come with their own set of strengths and weaknesses. On the positive side, they have the ability to adapt to various language tasks without requiring task-specific training. This is very different from traditional task-specific machine learning models. Their contextual understanding allows them to grasp nuances in language that previous models struggled with. Moreover, their generation capabilities enable them to produce human-like text for diverse purposes. One of their most impressive features is few-shot learning – the ability to adapt to new tasks with minimal examples.\n",
    "\n",
    "Yet, it's crucial to be aware of their limitations. Despite their impressive abilities, LLMs lack true real-world knowledge and may generate plausible-sounding but factually incorrect information. This is known as a hallucination They can inadvertently perpetuate societal biases present in their training data. From a practical standpoint, they demand significant computational resources to train and run. It's also important to note that while they can mimic reasoning, they don't truly \"understand\" in a human sense, and they may struggle with consistency, potentially providing different answers to the same question asked in different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's an API?\n",
    "\n",
    "An API, or Application Programming Interface, is essentially a messenger that allows different software applications to talk to each other. The role of the API is to effectively communicate requests from a user to an endpoint, for that endpoint to do something with that request, and then return the results to the user.\n",
    "\n",
    "A common way to think about and understand APIs is to consider a restaurant. In this scenario, you are the customer (your program), and you want to order food (data or services). The food server (API) takes your order to the kitchen (computer server), the kitchen prepares your food (processes your request), and the food server brings back your order (returns the response). This is exactly how APIs work - they act as intermediaries that handle requests and responses between different systems.\n",
    "\n",
    "![Restaurant API Example](../assets/images/api.jpg)\n",
    "\n",
    "APIs have several important characteristics. First, they provide standardized communication - they offer consistent ways to request and receive data, with specific endpoints (like URLs) for different types of requests, and clear rules about what data can be sent and received. Second, they provide abstraction - they hide the complex inner workings of a service, so you don't need to know how everything works behind the scenes. You just need to know how to make requests and handle the responses. Third, they include security measures - most APIs require authentication (like API keys), control access to data and services, and often limit how many requests you can make.\n",
    "\n",
    "If we think about this in the example of our restaurant above, we can make a slight modification to the restaurant. Instead of it being something like an Outback Steakhouse, let's imagine it is a modern-day speakeasy.\n",
    "\n",
    "![Wikipedia Speakeasy Club 21](https://upload.wikimedia.org/wikipedia/commons/0/00/21Club.JPG)\n",
    "\n",
    "In order to get into this location, you need to know a special password before you can ever request an order inside. To get inside, you provide that password to someone at the frontdoor (an Authenticator) whose sole job it is to verify that you have permission to access the restaurant (endpoint). Once validated, you can make the request. But the restaurant only has a limited amount of resources (computer). You cannot just keep ordering food after all the ingredients in the restaurant are depleted. To prevent this from happening, the restaurant might put realistic limitations on your access. You cannot, for example, purchase 1,000 plates of food. That would prevent other cusomters from getting food and may make the business look bad on Yelp.\n",
    "\n",
    "You encounter APIs in many different forms. Web APIs, like the Twitter API or Google Maps API, allow websites to communicate with servers. Library APIs provide functions and methods for programming languages, while Operating System APIs enable applications to interact with the operating system.\n",
    "\n",
    "When it comes to working with Large Language Models (LLMs), APIs are particularly important. They provide a structured way to interact with these powerful models, allowing you to send text prompts and receive responses. They make it possible to integrate AI capabilities into your applications, and they help manage costs and usage through features like rate limiting and authentication. Without APIs, it would be much more difficult to harness the power of LLMs in your projects.\n",
    "\n",
    "APIs are also necessary because many who use LLMs do not have the hardware to use the model locally (on their own machine) and in many cases LLMs are proprietary (closed-sourice), meaning they can only be used via APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the OpenAI API\n",
    "\n",
    "In order to setup your OpenAI API Key, you first need to create an account and follow the steps below (as seen in the following gif).\n",
    "\n",
    "![Gif of how to create OpenAI API Key](../assets/gifs/openai-api.gif)\n",
    "\n",
    "### Getting an API Key\n",
    "\n",
    "1) [platform.openai.com](platform.openai.com)\n",
    "2) Profile (top-right corner)\n",
    "3) Your Profile\n",
    "4) Admin Keys\n",
    "5) Create new API Key\n",
    "6) Type API key name and Select Project\n",
    "7) Click \"Craete API Key\"\n",
    "8) Copy API Key\n",
    "9) Paste either into notebook (**VERY INSECURE!!!**) or into a .env file (Make sure to keep .env in .gitignore).\n",
    "\n",
    "### Setting up Billing\n",
    "\n",
    "Once you have your API key, you will need to visit Billing to setup your credit card."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making an API Call with OpenAI\n",
    "\n",
    "When we make an API call, we are sending our request over the internet to a specific server. To use the OpenAI API, we need to do the exact same thing. Fortunately, we don't have to manually code this request out in complex JSON. Instead, we can leverage the Python `openai` package that we installed above.\n",
    "\n",
    "The `openai` package handles a lot of the complex structuring of a request for us, including passing our API key to the server for verification and structurin the requests correctly. To do this, we first need to import the required libraries. Let's import the `OpenAI` class from the `openai` package and `os` (which we can use to read our .env file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have imported the libraries, we can begin to use them. First, we need to connect to OpenAI via the main `OpenAI` client class. For basic use-cases this only requires a single `api_key` variable. This will not work without an API key. If you havne't made one yet, please scroll up and following the instructions above.\n",
    "\n",
    "You will either paste your API key directly into this cell (THIS IS VERY INSECURE as you may accidently commit it to the notebook and upload it) or (better option) retrieve it via `os` from a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    # api_key=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've connected, let's verify that the class is in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.OpenAI at 0x112bcaea0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exellent! Now, assuming our API key is valid, we can go ahead and structure our first response via the `responses.create` method. This will take two essential arguments: the model that we wish to use and the input that we wish to pass. This is a very simple example. We will expand on this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Hello. Who is William Mattingly?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our response, we can access the output as raw text via the `output_text` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "William Mattingly is a Digital Humanities researcher and professor. He is known for his work in the intersection of medieval studies and artificial intelligence. He has conducted research on using AI for historical and literary analysis, and he has taught courses related to these topics. If you have more specific questions about his work or contributions, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our outputs will be slightly different. I've chosen this particular prompt for a specific reason. I'm represented enough on the internet to be included in the model's knowledge but just barely. This makes information on me inside an LLM to be highly unpredictable. In my output, it calls me a `professor`, but this is not true. We will learn about this mistake below as we talk more about hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EXERCISE 1** (5 Minutes)\n",
    "\n",
    "Spend the next 10 minutes familiarizing yourself with the syntax of the above code. Test out different prompts. Design a test for the LLM. Try and get it to generate a specific output. You can be createive here! Your goal is to get the model to generate the same response consistently five times. *Please do not be discouraged if you can't do this.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of a Prompt\n",
    "\n",
    "A **prompt** is the input text that we provide to an LLM to generate a response. It can be a question, statement, instruction, or any other form of text that we want the model to process and respond to. The quality and structure of the prompt greatly influences the output we receive from the model.\n",
    "\n",
    "While a basic prompt has a sinlge input (like we saw above), a typical prompt will include a sequence of messages. We typically create these as messages as a separate variable. A good way to think about messages is as the interface we know as ChatGPT. When you first create a ChatGPT session, there is already a system prompt engaged behind the scenes.\n",
    "\n",
    "The system prompt is responsible for defining how the model will behave when engaging with the user. The **user** is something the human submits to the LLM. As we will see below, there is another role called the **assistant**, but we will address that later. For now, let's consider this basic arrangement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You write creative short stories.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write three sentences about sharks.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=basic_messages\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that instead of using `client.response.create` we are using `client.chat.completions.craete` this is more typical of how you interact with LLMs via an API as it offers a lot more control over how the LLM behaves. In order to access the responses, we also need slightly different syntax. The example below shows you how to get the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharks glide through the ocean with a grace that belies their size, their sleek fins slicing through the water like knives through silk. Their eyes, always scanning the infinite blue, hold secrets of the deep that have remained untold for centuries. Despite their predatory reputation, they possess an ancient elegance, a reminder of the wild beauty that has swum the Earth's waters long before we walked its lands.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(id='gpt-4o-audio-preview-2024-12-17', created=1734034239, object='model', owned_by='system')\n",
      "Model(id='dall-e-3', created=1698785189, object='model', owned_by='system')\n",
      "Model(id='dall-e-2', created=1698798177, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-audio-preview-2024-10-01', created=1727389042, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-realtime-preview-2024-10-01', created=1727131766, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-realtime-preview', created=1727659998, object='model', owned_by='system')\n",
      "Model(id='babbage-002', created=1692634615, object='model', owned_by='system')\n",
      "Model(id='o3-mini-2025-01-31', created=1738010200, object='model', owned_by='system')\n",
      "Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system')\n",
      "Model(id='o3-mini', created=1737146383, object='model', owned_by='system')\n",
      "Model(id='text-embedding-3-large', created=1705953180, object='model', owned_by='system')\n",
      "Model(id='gpt-4', created=1687882411, object='model', owned_by='openai')\n",
      "Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal')\n",
      "Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-audio-preview', created=1734387424, object='model', owned_by='system')\n",
      "Model(id='gpt-4-0125-preview', created=1706037612, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-audio-preview', created=1727460443, object='model', owned_by='system')\n",
      "Model(id='gpt-4-turbo-preview', created=1706037777, object='model', owned_by='system')\n",
      "Model(id='o1-preview-2024-09-12', created=1725648865, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-realtime-preview', created=1734387380, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-realtime-preview-2024-12-17', created=1734112601, object='model', owned_by='system')\n",
      "Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-search-preview', created=1741391161, object='model', owned_by='system')\n",
      "Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system')\n",
      "Model(id='davinci-002', created=1692634301, object='model', owned_by='system')\n",
      "Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system')\n",
      "Model(id='gpt-4-turbo', created=1712361441, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-realtime-preview-2024-12-17', created=1733945430, object='model', owned_by='system')\n",
      "Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system')\n",
      "Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai')\n",
      "Model(id='chatgpt-4o-latest', created=1723515131, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-search-preview-2025-03-11', created=1741390858, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-2024-11-20', created=1739331543, object='model', owned_by='system')\n",
      "Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal')\n",
      "Model(id='gpt-4o-2024-05-13', created=1715368132, object='model', owned_by='system')\n",
      "Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal')\n",
      "Model(id='gpt-4-turbo-2024-04-09', created=1712601677, object='model', owned_by='system')\n",
      "Model(id='gpt-4-1106-preview', created=1698957206, object='model', owned_by='system')\n",
      "Model(id='o1-preview', created=1725648897, object='model', owned_by='system')\n",
      "Model(id='gpt-4-0613', created=1686588896, object='model', owned_by='openai')\n",
      "Model(id='gpt-4o-search-preview', created=1741388720, object='model', owned_by='system')\n",
      "Model(id='gpt-4.5-preview', created=1740623059, object='model', owned_by='system')\n",
      "Model(id='o1-2024-12-17', created=1734326976, object='model', owned_by='system')\n",
      "Model(id='gpt-4.5-preview-2025-02-27', created=1740623304, object='model', owned_by='system')\n",
      "Model(id='o1', created=1734375816, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-search-preview-2025-03-11', created=1741388170, object='model', owned_by='system')\n",
      "Model(id='o1-pro', created=1742251791, object='model', owned_by='system')\n",
      "Model(id='o1-pro-2025-03-19', created=1742251504, object='model', owned_by='system')\n",
      "Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal')\n",
      "Model(id='omni-moderation-2024-09-26', created=1732734466, object='model', owned_by='system')\n",
      "Model(id='text-embedding-3-small', created=1705948997, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-tts', created=1742403959, object='model', owned_by='system')\n",
      "Model(id='gpt-4o', created=1715367049, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini', created=1721172741, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-2024-08-06', created=1722814719, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-transcribe', created=1742068463, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-2024-07-18', created=1721172717, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-transcribe', created=1742068596, object='model', owned_by='system')\n",
      "Model(id='o1-mini', created=1725649008, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-audio-preview-2024-12-17', created=1734115920, object='model', owned_by='system')\n",
      "Model(id='gpt-3.5-turbo-0125', created=1706048358, object='model', owned_by='system')\n",
      "Model(id='o1-mini-2024-09-12', created=1725648979, object='model', owned_by='system')\n",
      "Model(id='omni-moderation-latest', created=1731689265, object='model', owned_by='system')\n",
      "Model(id='ada:ft-personal:vulgate-2023-04-18-17-48-10', created=1681840090, object='model', owned_by='user-gjtyp9rdorharypswlxl0npb')\n",
      "Model(id='ada:ft-personal:vulgate-2023-04-18-16-45-55', created=1681836355, object='model', owned_by='user-gjtyp9rdorharypswlxl0npb')\n"
     ]
    }
   ],
   "source": [
    "models = client.models.list()\n",
    "for model in models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see a very basic story. Your stories will vary significantly from this one. Let's take a moment, though and see if we can control the style of output by changing the `system` prompt. What if we wanted our narrator to be a pirate, well we can control the model's behavior by changing the initial system prompt to include `in the style of a pirate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, the sea be home to many fearsome beasts, but none as jaw-droppin’ as the mighty shark with its glistening teeth like daggers ready to strike. These cunning creatures glide through the briny deep with a grace and power that even the most seasoned buccaneer can’t help but admire. Beware, for when the sun dips below the horizon and the waters grow dark, a shark's fin slicing through the waves be a sight to chill even the bravest sailor to the bone.\n"
     ]
    }
   ],
   "source": [
    "pirate_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You write creative short stories in the style of a pirate.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write three sentences about sharks.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=pirate_messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does this work? One of the things we will learn about LLms throughout this class is that they are dramatically shapped by context. While this example is fun, let's take a sneak look at something we will examine in more depth later on. Imagine I wanted to get a quick biography of Michael Jordan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Jordan is a former professional basketball player widely regarded as one of the greatest athletes in the history of the sport. He played the majority of his career with the Chicago Bulls in the NBA, where he won six championships and earned the NBA Finals MVP award six times. Beyond his remarkable sports career, Jordan is also a successful businessman and the principal owner and chairman of the Charlotte Hornets.\n"
     ]
    }
   ],
   "source": [
    "mj_basic_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You write factual short biographies.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write three sentences about Michael Jordan.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=mj_basic_messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait... What? That's not the Michael Jordan I wanted! I wanted a biography of this [Michael Jordan](https://en.wikipedia.org/wiki/Michael_Jordan_(mycologist)). Let's fix this by adding a bit of context to the user role in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Jordan, not to be confused with the basketball legend, is a British mycologist known for his contributions to the field of mushroom science. He founded the Association of British Fungal Groups, which aims to promote the study and conservation of fungi. Jordan is also an author of various books on mushrooms, providing valuable resources for both amateur and professional mycologists.\n"
     ]
    }
   ],
   "source": [
    "mj_context_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You write factual short biographies.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write three sentences about Michael Jordan in the context of mushroom science.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=mj_context_messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will learn more about context later this week, but this is a quick demonstration of how radically context can influence an LLMs output. This is especially important in the humanities as we often times work with material that is on the periphery of knowledge of these LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Classification\n",
    "\n",
    "One of the most powerful aspects of LLMs is their ability to perform \"zero-shot\" learning - they can tackle new tasks without explicit training on specific examples. This makes them incredibly versatile tools for humanities research. For instance, they can analyze historical texts, identify patterns across large corpora, and even help with translation and transcription of historical documents.\n",
    "\n",
    "However, it's crucial to understand their limitations. LLMs don't truly \"understand\" language in the way humans do - they operate by identifying statistical patterns in their training data. This can lead to \"hallucinations\" where they generate false or misleading information that sounds plausible. They can also perpetuate and amplify biases present in their training data, which is particularly concerning for humanities research where historical accuracy and cultural sensitivity are paramount.\n",
    "\n",
    "In the humanities, LLMs are proving to be valuable tools for several applications. They can help scholars analyze large historical text corpora, identify named entities (people, places, events), discover patterns and themes across collections, and assist with the translation and transcription of historical documents. They can also serve as research assistants, helping scholars explore connections and generate research questions.\n",
    "\n",
    "Let's consider this fun example. Imagine we wanted to classify a string of text which contains a person's name, degree, and maybe some extra information. We specifically want to classify them by the type of degree that they have, for example, PhD or MD, etc. Let's try and use an LLM to prompt this without any examples. This is known as zero-shot classification, where we just ask the model to classify something with no examples given. I'm going to run this three times to demonstrate how varied our responses are each time we prompt the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Samantha Bower can be classified as an academic or historian. With a Ph.D. in History, she is likely involved in education, research, or scholarly activities related to historical studies.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Classify the following person. Dr. Samantha Bower, Ph.D. in History\"\n",
    ")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Samantha Bower can be classified as an academic or historian, given her Ph.D. in History. She likely engages in activities related to research, teaching, and scholarly writing in the field of history.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Classify the following person. Dr. Samantha Bower, Ph.D. in History\"\n",
    ")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Samantha Bower can be classified as an academic or historian, given her Ph.D. in History.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Classify the following person. Dr. Samantha Bower, Ph.D. in History\"\n",
    ")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While none of these responses are wrong, they aren't *exactly* what I wanted. I really just want to know the specific degree. How can I improve the results? With examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EXERCISE 2** (10 minutes)\n",
    "\n",
    "Without using messages, try and use the example above to get the model to just give you PhD from this example above. Once successful, try and do the same thing with the same prompt but just change the `Dr. Samantha Bower, Ph.D in History` part of the prompt. **IMPORTANT** to pass this exercise you must get the model to give you that same response three times. (Please do not scroll down.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Classification\n",
    "\n",
    "Sometimes solving a particular classification class cannot be done with zero-classification alone. While the above examples work in a broad sense, they lack consistency. We aren't getting classes in the traditional sense of the word in machine learning terms. Instead, we are getting natural language descriptions of the person. While useful, this may not be precisely what we need.\n",
    "\n",
    "In these circumstances, we can still leverage the LLM's broad knowledge of language by guiding it better through examples. This is known as single-shot and few-shot classification. We will learn about the code in more depth in a later notebook. For now, it's important to understand that we are leveraging examples for *how* we want to classify the texts, that is as an output that is simply the person's degree, not a natural language response.\n",
    "\n",
    "Few-shot can not only improve the consistency of the response, it can even improve the response as a whole. In the example below, we are adding a new role to the sequence of messages **assistant**. The **assistant** is the LLM. Here, we are simulating an ideal conversation. Just like before, we start with a system prompt, then we pose a prompt from the user with a guided response from the LLM. We use another example, but note how these two examples show two different types of input data (different degrees). When constructing few-shot examples, try and use diverse material. This allows a model to generalize (or make a prediction) better on unseen data. Finally, we conclude the sequence of messages with our actual text we need classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ph.D.\n"
     ]
    }
   ],
   "source": [
    "long_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an assistant that classifies people by their degrees.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Classify the following person. Dr. Frank Alvez, M.D. with specialty in Cardiology\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"M.D.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Classify the following person. Jeff Stanton, Ph.D in Philosophy\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Ph.D.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Classify the following person. Dr. Samantha Bower, Ph.D. in History\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=long_messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can rerun this cell as much as you like, but rarely will it diverge from this output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 3** (5 Minutes)\n",
    "\n",
    "Instead of getting the model to say Ph.D. Try and get the model to say PhD."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
